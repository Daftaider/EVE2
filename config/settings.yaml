llm:
  # Path to the GGUF model file
  model_path: src/models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
  # Context window size
  context_window: 2048
  # Maximum tokens to generate in response
  max_tokens: 256
  # Sampling temperature
  temperature: 0.7 